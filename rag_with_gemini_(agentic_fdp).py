# -*- coding: utf-8 -*-
"""RAG with Gemini - (Agentic FDP).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eGTQQgR01fMApsevNd-qHVdZHmiXOISN
"""

# Install dependencies
!pip install pymupdf sentence-transformers tiktoken

# Upload and extract text from PDF
import fitz  # PyMuPDF
from google.colab import files # Specific to Google Colab

uploaded = files.upload()
pdf_text = ""
for fname in uploaded:
    with fitz.open(fname) as doc: # Opens the PDF
        for page in doc: # Iterates through each page
            pdf_text += page.get_text() # Appends extracted text
print("PDF text loaded and extracted.")

# Chunk text using token limits
import tiktoken


def chunk_text(text, max_tokens=300):
    enc = tiktoken.get_encoding("cl100k_base") # Load tokenizer
    words = text.split(". ") # Simple sentence split
    chunks, current = [], ""
    for sentence in words:
        # Check if adding the sentence exceeds the token limit
        if len(enc.encode(current + sentence)) < max_tokens:
            current += sentence + ". "
        else:
            chunks.append(current.strip()) # Add finished chunk
            current = sentence + ". " # Start new chunk
    if current: # Add any remaining text
        chunks.append(current.strip())
    return chunks


chunks = chunk_text(pdf_text)
print(f"Created {len(chunks)} text chunks.")

# Embed chunks using sentence-transformers
from sentence_transformers import SentenceTransformer, util
import torch


# Load a pre-trained embedding model
embedder = SentenceTransformer("all-MiniLM-L6-v2")
# Convert text chunks into numerical vectors (embeddings)
chunk_embeddings = embedder.encode(chunks, convert_to_tensor=True)
print(f"Created {chunk_embeddings.shape[0]} embeddings.")

# Define retrieval function
def retrieve_relevant_chunks(query, top_k=3):
    query_emb = embedder.encode(query, convert_to_tensor=True)
    scores = util.pytorch_cos_sim(query_emb, chunk_embeddings)[0]
    top_results = torch.topk(scores, k=min(top_k, len(chunks)))


    return [chunks[i] for i in top_results.indices]

# Use Gemini API with retrieved context
from google import generativeai as genai # Renamed to avoid conflict
from google.generativeai import types
import os

# REPLACE THE $$$$ below with your API Key
os.environ["MY_GEMINI_API_KEY"] = "AIzaSyCznysnsApBt-_PzI-KS156EwwSCR6g2Z8"
genai.configure(api_key=os.environ["MY_GEMINI_API_KEY"])


# Initialize the model object
rag_model = genai.GenerativeModel('gemini-2.0-flash-lite')

SYSTEM_INSTRUCTION = "Use only the context provided to answer the question. If the answer is not in the context, say 'I cannot answer based on the provided document. PLEASE Translate it in tamil'"


def gen_content_with_rag(query: str):
    retrieved = retrieve_relevant_chunks(query)
    context = "\n\n".join(retrieved)
    prompt_to_model = f"Context:\n{context}\n\nQuestion: {query}"
    response = rag_model.generate_content([prompt_to_model, SYSTEM_INSTRUCTION])
    print(f"Query: {query}")
    print(f"Gemini RAG Answer: {response.text}")

gen_content_with_rag("What is the default password while setting the TPM versions?")

gen_content_with_rag("What are the status LED lights for?")

# Use Gemini API with retrieved context
from google import generativeai as genai # Renamed to avoid conflict
from google.generativeai import types
import os

# REPLACE THE $$$$ below with your API Key
os.environ["MY_GEMINI_API_KEY"] = "AIzaSyCznysnsApBt-_PzI-KS156EwwSCR6g2Z8"
genai.configure(api_key=os.environ["MY_GEMINI_API_KEY"])


# Initialize the model object
rag_model = genai.GenerativeModel('gemini-2.0-flash-lite')

SYSTEM_INSTRUCTION = "Use only the context provided to answer the question.If the answer is not in the context, say 'I cannot answer based on the provided document'. based on the question, generate 5 multiple choice questions in the format: Question, option1: option2, Option2: option2, .... correct option"

def gen_content_with_rag(query: str):
    retrieved = retrieve_relevant_chunks(query)
    context = "\n\n".join(retrieved)
    prompt_to_model = f"Context:\n{context}\n\nQuestion: {query}"
    response = rag_model.generate_content([prompt_to_model, SYSTEM_INSTRUCTION])
    print(f"Query: {query}")
    print(f"Gemini RAG Answer: {response.text}")

gen_content_with_rag("What is the default password while setting the TPM versions?")

gen_content_with_rag("What are the status LED lights for?")